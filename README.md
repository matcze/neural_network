Neural Network from Scratch (NumPy Only)

This project implements a fully connected neural network from scratch using NumPy, without relying on any deep learning frameworks. The network is trained on the MNIST dataset for handwritten digit classification.

Problem

MNIST Classification

Input images: 28 Ã— 28 pixels

Flattened input size: 784 features

Output classes: 10 (digits 0â€“9)

Task type: Multiclass classification

Neural Network Architecture

The network consists of three layers:

Input Layer

784 nodes (one per pixel)

No trainable parameters

Hidden Layer

10 neurons

Activation function: ReLU

Output Layer

10 neurons (one per class)

Activation function: Softmax

Forward Propagation

Let:

ğ´
[
0
]
=
ğ‘‹
A[0]=X (input data)

ğ‘Š
[
ğ‘™
]
W[l] = weights of layer 
ğ‘™
l

ğ‘
[
ğ‘™
]
b[l] = bias of layer 
ğ‘™
l

Hidden Layer:

ğ‘
[
1
]
=
ğ‘Š
[
1
]
â‹…
ğ´
[
0
]
+
ğ‘
[
1
]
Z[1]=W[1]â‹…A[0]+b[1]
ğ´
[
1
]
=
ğ‘…
ğ‘’
ğ¿
ğ‘ˆ
(
ğ‘
[
1
]
)
A[1]=ReLU(Z[1])

Output Layer:

ğ‘
[
2
]
=
ğ‘Š
[
2
]
â‹…
ğ´
[
1
]
+
ğ‘
[
2
]
Z[2]=W[2]â‹…A[1]+b[2]
ğ´
[
2
]
=
ğ‘ 
ğ‘œ
ğ‘“
ğ‘¡
ğ‘š
ğ‘
ğ‘¥
(
ğ‘
[
2
]
)
A[2]=softmax(Z[2])
Activation Functions

ReLU (Rectified Linear Unit)

ğ‘…
ğ‘’
ğ¿
ğ‘ˆ
(
ğ‘¥
)
=
{
ğ‘¥
	
if 
ğ‘¥
>
0


0
	
if 
ğ‘¥
â‰¤
0
ReLU(x)={
x
0
	â€‹

if x>0
if xâ‰¤0
	â€‹


Introduces non-linearity into the model

Softmax

ğ‘ 
ğ‘œ
ğ‘“
ğ‘¡
ğ‘š
ğ‘
ğ‘¥
(
ğ‘§
ğ‘–
)
=
ğ‘’
ğ‘§
ğ‘–
âˆ‘
ğ‘—
=
1
ğ¾
ğ‘’
ğ‘§
ğ‘—
softmax(z
i
	â€‹

)=
âˆ‘
j=1
K
	â€‹

e
z
j
	â€‹

e
z
i
	â€‹

	â€‹


Each output value is between 0 and 1

Outputs sum to 1

Used for multiclass classification

Note: Sigmoid is typically used for binary classification, while softmax is preferred for multiclass problems.

Backpropagation

Backpropagation computes gradients of the loss with respect to weights and biases.

Let:

ğ‘š
m = number of training examples

ğ‘Œ
Y = true labels (one-hot encoded)

ğ´
[
2
]
A[2] = predicted probabilities

Output Layer:

ğ‘‘
ğ‘
[
2
]
=
ğ´
[
2
]
âˆ’
ğ‘Œ
dZ[2]=A[2]âˆ’Y
ğ‘‘
ğ‘Š
[
2
]
=
1
ğ‘š
ğ‘‘
ğ‘
[
2
]
â‹…
ğ´
[
1
]
ğ‘‡
dW[2]=
m
1
	â€‹

dZ[2]â‹…A[1]
T
ğ‘‘
ğ‘
[
2
]
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
[
2
]
db[2]=
m
1
	â€‹

âˆ‘dZ[2]

Hidden Layer:

ğ‘‘
ğ‘
[
1
]
=
(
ğ‘Š
[
2
]
ğ‘‡
â‹…
ğ‘‘
ğ‘
[
2
]
)
âˆ—
ğ‘…
ğ‘’
ğ¿
ğ‘ˆ
â€²
(
ğ‘
[
1
]
)
dZ[1]=(W[2]
T
â‹…dZ[2])âˆ—ReLU
â€²
(Z[1])
ğ‘‘
ğ‘Š
[
1
]
=
1
ğ‘š
ğ‘‘
ğ‘
[
1
]
â‹…
ğ´
[
0
]
ğ‘‡
dW[1]=
m
1
	â€‹

dZ[1]â‹…A[0]
T
ğ‘‘
ğ‘
[
1
]
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
[
1
]
db[1]=
m
1
	â€‹

âˆ‘dZ[1]
Parameter Update (Gradient Descent)
ğ‘Š
[
1
]
=
ğ‘Š
[
1
]
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘Š
[
1
]
W[1]=W[1]âˆ’Î±â‹…dW[1]
ğ‘
[
1
]
=
ğ‘
[
1
]
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘
[
1
]
b[1]=b[1]âˆ’Î±â‹…db[1]
ğ‘Š
[
2
]
=
ğ‘Š
[
2
]
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘Š
[
2
]
W[2]=W[2]âˆ’Î±â‹…dW[2]
ğ‘
[
2
]
=
ğ‘
[
2
]
âˆ’
ğ›¼
â‹…
ğ‘‘
ğ‘
[
2
]
b[2]=b[2]âˆ’Î±â‹…db[2]

Where 
ğ›¼
Î± is the learning rate, a user-defined hyperparameter.

Summary

This project demonstrates:

Forward propagation using NumPy

Implementation of ReLU and Softmax activations

Backpropagation using matrix operations

Gradient descent optimization

Multiclass classification with one-hot encoding